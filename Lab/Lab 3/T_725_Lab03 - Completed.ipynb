{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gb8JZdRCj6L"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 3\n",
        "In today's lab, we will be working with logistic regression and part-of-speech tagging, as well as word embeddings.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook.\n",
        "\n",
        "Start by installing the Gensim library:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "wd3H8ibx0pIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1394d334-89fb-40ee-a256-72835836f5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL4Pb6jbR0_j"
      },
      "source": [
        "## Extracting numerical features from text\n",
        "Machine learning algorithms generally only accept numerical input, meaning that we must represent all features numerically. For example, to classify a single sentence, we might pass a classifier a list of word counts in that sentence, or a list of `True` and `False` values (which have numerical values of 1 and 0, respectively), representing the presence or absence of particular words.\n",
        "\n",
        "[Scikit-learn](https://scikit-learn.org/stable/) is a popular machine learning library for Python that implements a wide variety of machine learning algorithms, including naive Bayesian and logistic regression. It also offers a convenient way to extract numerical features from text, for example with the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class. `CountVectorizer` is used to generate feature vectors containing character or word n-gram counts for any n within a given range (e.g., `ngram_range=(2, 2)` for only bigrams, or `ngram_range(1, 3)` for unigrams, bigrams and trigrams). The `CountVectorizer` has an attribute called `analyzer` that can be set to 'char' for character n-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bMHZMO5TNbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1cc4bd-1053-48fd-9de3-2c69a60d79c1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize a vectorizer that counts word bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Count all bigrams in the sentences and create a feature vector\n",
        "sentences = [\"It was the best of times, it was the worst of times,\",\n",
        "             \"it was the age of wisdom, it was the age of foolishness,\"]\n",
        "\n",
        "vector = vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Bigrams:\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nFeatures:\")\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: ['age of' 'best of' 'it was' 'of foolishness' 'of times' 'of wisdom'\n",
            " 'the age' 'the best' 'the worst' 'times it' 'was the' 'wisdom it'\n",
            " 'worst of']\n",
            "\n",
            "Features:\n",
            "[[0 1 2 0 2 0 0 1 1 1 2 0 1]\n",
            " [2 0 2 1 0 1 2 0 0 0 2 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie-9LuaLX_Yw"
      },
      "source": [
        "Here, `vectorizer` created a matrix with 13 columns (one for each bigram) and two rows (one for each sentence). Each row consists of bigram counts for the corresponding sentence. For example, the first sentence has the bigram counts `[0 1 2 0 2 0 0 1 1 1 2 0 1]`, which means that it contains 0 instances of \"age of\", 1 instance of \"best of\", two instances of \"it was\", and so on (we can see which column represents which bigram with `vecorizer.get_feature_names()`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1Jb9cBxhTMh"
      },
      "source": [
        "## Creating training and test sets\n",
        "Scikit-learn lets us quickly split data into training and test sets with the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. Note that by convention, examples are generally denoted with a capital X while labels are denoted with a lowercase y. Let's create a training and test set for the subjectivity corpus from the NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YbXr6-KhYDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de166b5-8efb-4744-ca5d-6e00af7cdb87"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import subjectivity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download the subjectivity corpus and get the sentences for each category\n",
        "nltk.download('subjectivity')\n",
        "\n",
        "obj_fileids = subjectivity.fileids('obj')\n",
        "subj_fileids = subjectivity.fileids('subj')\n",
        "\n",
        "# Let's get the untokenized sentences from each category\n",
        "obj_sentences = subjectivity.raw(obj_fileids).splitlines()\n",
        "subj_sentences = subjectivity.raw(subj_fileids).splitlines()\n",
        "\n",
        "X = obj_sentences + subj_sentences\n",
        "y = ['obj'] * 5000 + ['subj'] * 5000\n",
        "\n",
        "# Create a word unigram count vectorizer and generate the feature vectors\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Create a training and test set (80%/20% split). This function always shuffles\n",
        "# the examples before making the split, but we can make sure that it always\n",
        "# shuffles them the same way by specifying a specific random_state value.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCTmsvXzgY-C"
      },
      "source": [
        "## Logistic regression\n",
        "We can create a logistic regression classifier with the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGffjsuFghoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53fc763-1cc3-47a4-d112-b544860ab551"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "subj_clf = LogisticRegression(solver='liblinear')\n",
        "subj_clf.fit(X_train, y_train)  # Train the model\n",
        "score = subj_clf.score(X_test, y_test)  # Evaluate the model on the test set\n",
        "print(\"Accuracy: {:.1%}\".format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZEznKZAuj1-"
      },
      "source": [
        "Our logistic regression classifier obtains an accuracy of 90.2%, which is quite a bit higher than the accuracy obtained by NLTK's naive Bayes classifier in a previous lab.\n",
        "\n",
        "Once the classifier is trained, we can use it to classify new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ev9cbxGunWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93cde0b-a6b7-4fd5-dc2f-6c836e834222"
      },
      "source": [
        "example_sentences = [\n",
        "  \"Monty Python's Flying Circus, the British comedy group which gained fame via\\\n",
        "   BBC-TV, send-up Arthurian legend, performed in whimsical fashion with Graham\\\n",
        "   Chapman an effective straight man as King Arthur.\",\n",
        "  \"The funniest movie of 1975 and probably the silliest movie ever made.\"\n",
        "]\n",
        "\n",
        "features = vectorizer.transform(example_sentences)\n",
        "subj_clf.predict(features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['obj', 'subj'], dtype='<U4')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKiN7bhwfk0j"
      },
      "source": [
        "## Pipelines\n",
        "Instead of having to call `vectorizer.transform()` every time we use the classifier, we can create a `Pipeline` that automatically extracts features for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMuh7usegvTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37794299-5c94-43d9-eff4-cf2d6adfcc76"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# The feature vectors are automatically created\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "score = pipeline.score(X_test, y_test)\n",
        "print(\"Accuracy: {:.1%}\".format(score))\n",
        "\n",
        "pipeline.predict(example_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.2%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['obj', 'subj'], dtype='<U4')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8AqtdrsdfZ0"
      },
      "source": [
        "## Creating word embeddings\n",
        "[Gensim](https://radimrehurek.com/gensim/) is a Python library that makes it easy to generate and work with word embeddings.\n",
        "\n",
        "Let's start by supressing some warnings from Gensim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7ElxTOtl6UQ"
      },
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress some warnings from Gensim about deprecated functions\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create word2vec embeddings for NLTK's movie review corpus:"
      ],
      "metadata": {
        "id": "fs583ebKoiXK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvQMJDrdvdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14da1313-e130-4336-95d0-f9cf36dde3af"
      },
      "source": [
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import movie_reviews\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "sents = movie_reviews.sents()\n",
        "movie_embeddings = Word2Vec(sents, epochs=1, min_count=5, vector_size=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHF4ReN-DVr3"
      },
      "source": [
        "What does the vector for *actor* look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpOAPLLktUmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c3f704-0422-4c7f-c3aa-de7c86d63fd8"
      },
      "source": [
        "movie_embeddings.wv['actor']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.13427263,  0.02140188, -0.27704212, -0.08291544, -0.31226793,\n",
              "       -0.31160232,  0.9277424 ,  0.6497605 , -1.1103605 , -0.40497503,\n",
              "       -0.1018837 , -0.663367  ,  0.6477774 ,  0.51049685, -0.35326418,\n",
              "        0.4696123 ,  0.49662134,  0.363332  , -1.1534629 , -0.63883495,\n",
              "        0.46620882,  0.57983404,  1.0120492 , -0.11971117,  0.43905967,\n",
              "        0.40311086,  0.01586455,  0.05595597, -0.5313467 , -0.04370743,\n",
              "       -0.08313575, -0.6258688 ,  0.1908502 , -0.12921032, -0.21366651,\n",
              "        0.23270613,  0.5246473 ,  0.0096391 ,  0.10452011, -0.22444108,\n",
              "        0.5511495 , -0.43369132,  0.42714193,  0.23117904,  1.3434731 ,\n",
              "        0.0345616 , -0.04136414, -0.7252808 ,  0.42802057,  0.41010264],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEfsQ7TJCutn"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before 23:59, Friday Sept 12th. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Zayf9rC2cP"
      },
      "source": [
        "## Question 1\n",
        "The NLTK includes a copy of the *Universal Declaration of Human Rights* (UDHR) in over 300 languages, including Icelandic, Norwegian, Swedish, Danish, Finnish and Faroese.\n",
        "\n",
        "Create a `Pipeline` with a `CountVectorizer` and a `LogisticRegression` classifier that satisfies the following requirements:\n",
        "\n",
        "The `CountVectorizer` should:\n",
        "* Create character-level n-grams.\n",
        "* Generate unigram, bigram and trigram counts.\n",
        "\n",
        "The `LogisticRegression` classifier should:\n",
        "* Use the `liblinear` solver.\n",
        "\n",
        "Refer to Scikit-learn's reference for the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for information on possible parameters.\n",
        "\n",
        "Once you've created the pipeline, train it using the `train_udhr(pipeline)` function below, which returns the test examples and labels (and should not be modified). Report the accuracy of the classifier, and try making predictions on a few sentences from these languages, for example from Wikipedia ([is](https://is.wikipedia.org/wiki/Fors%C3%AD%C3%B0a), [no](https://no.wikipedia.org/wiki/Portal:Forside), [se](https://sv.wikipedia.org/wiki/Portal:Huvudsida), [da](https://da.wikipedia.org/wiki/Forside), [fi](https://fi.wikipedia.org/wiki/Wikipedia:Etusivu), [fo](https://fo.wikipedia.org/wiki/Fors%C3%AD%C3%B0a)). One sentence from each language is enough. Does the classifier perform as well as you would expect, given the reported accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gypk375K55Zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7377e35-f49b-40c2-c67d-25459653e4e2"
      },
      "source": [
        "# Don't change anything in this code cell\n",
        "import random\n",
        "from nltk.corpus import udhr\n",
        "nltk.download('udhr')\n",
        "\n",
        "def train_udhr(pipeline):\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  # The UDHR is quite small, so let's create 1,000 \"fake\" sentences in each\n",
        "  # language by randomly stringing together 3-15 words.\n",
        "  for lang in languages:\n",
        "    words = udhr.words(lang)\n",
        "    sents = [\" \".join(random.choices(words, k=random.randint(3, 15))) for x in range(1000)]\n",
        "    X.extend(sents)\n",
        "    y += [lang] * len(sents)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                      y,\n",
        "                                                      test_size=0.1,\n",
        "                                                      random_state=42)\n",
        "\n",
        "  # Train the classifier\n",
        "  pipeline.fit(X_train, y_train)\n",
        "  return X_test, y_test\n",
        "\n",
        "languages = ['Icelandic_Yslenska-Latin1',\n",
        "             'Norwegian-Latin1',\n",
        "             'Swedish_Svenska-Latin1',\n",
        "             'Danish_Dansk-Latin1',\n",
        "             'Finnish_Suomi-Latin1',\n",
        "             'Faroese-Latin1']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Package udhr is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-sM02NcDrZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b734044-c796-4724-9045-88986c5b9f73"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1, 3), analyzer=\"char\")),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "X_test, Y_test = train_udhr(pipeline)\n",
        "score = pipeline.score(X_test, Y_test)\n",
        "print(\"Accuracy: {:.1%}\".format(score))\n",
        "\n",
        "example_udhr = [\n",
        "  \"St. Gallen er sjöunda stærsta borgin í Sviss og er höfuðborg samnefndrar\\\n",
        "   kantónu.\",\n",
        "\n",
        "  \"Renessanse og reformasjon i Europa foregikk i en periode fra omtrent 1450 til\\\n",
        "   1556, innenfor den større tidsperioden omtalt som tidlig moderne tid. \",\n",
        "\n",
        "  \"Gråsparv (Passer domesticus) är en fågel som tillhör familjen sparvfinkar\\\n",
        "   (Passeridae). Gråsparven är spridd över stora delar av Europa och Asien, men\\\n",
        "    har även av människan introducerats till Amerika, Afrika och Australien och\\\n",
        "   är idag en av världens mest spridda fågelarter. \",\n",
        "\n",
        "  \"Skagen er ligeledes kendt for sin fiskerihavn og for den årlige Skagen\\\n",
        "   Festival, der fandt sted første gang i 1971.\",\n",
        "\n",
        "  \"Politiikan vaikuttaja Charlie Kirk surmattiin ampumavälikohtauksessa Utahissa.\",\n",
        "\n",
        "  \"Tann 64 ára gamli norski høvundurin Jon Fosse (myndin)\\\n",
        "  fær heiðursløn Nobels í bókmentum fyri 2023.\"\n",
        "]\n",
        "\n",
        "print(pipeline.predict(example_udhr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.8%\n",
            "['Icelandic_Yslenska-Latin1' 'Norwegian-Latin1' 'Swedish_Svenska-Latin1'\n",
            " 'Danish_Dansk-Latin1' 'Finnish_Suomi-Latin1' 'Faroese-Latin1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwetpCxTCwXB"
      },
      "source": [
        "## Question 2\n",
        "The logistic regression classifier below tries to determine which of the following tags should be assigned to a given word:\n",
        "* **NP** (proper nouns, singular),\n",
        "* **NP\\$** (proper nouns, singular and possessive),\n",
        "* **VBG** (verbs, present participle) or\n",
        "* **VBD** (verbs, past tense).\n",
        "\n",
        "The classifier makes its determination solely on characteristics of the word itself and does not make use of any contextual features. The function `extract_features(word)` extracts a list of numerical features from each word, currently only the length of a word and whether or not it ends with \"r\". Using these features, the classifier obtains an accuracy of 37.1%, which is quite poor. Replace the features that the `exctract_features()` function generates with your own. Use Python's [string methods](https://docs.python.org/3/library/stdtypes.html#string-methods) to generate the features, and try to get at least 99% accuracy.\n",
        "\n",
        "**Remember**: each feature must be numerical (or `True`/`False`), and don't forget to add a comma after each feature in the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-be0psqueELC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72a455c-49db-4bb0-86d5-40f0eb5202b6"
      },
      "source": [
        "# Don't change anything in this code cell\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "def get_brown_tags(tag):\n",
        "  return sorted({w for s in brown_train for w, t in s if t == tag})\n",
        "\n",
        "def train_model():\n",
        "  # Create the training set\n",
        "  word_list = [word for tag_words in words for word in tag_words]\n",
        "  X = [extract_features(word) for word in word_list]\n",
        "  y = [tag for tag, tag_words in zip(tags, words) for word in tag_words]\n",
        "\n",
        "  # Train and evaluate the classifier\n",
        "  log_clf = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "  log_clf.fit(X, y)\n",
        "  print(\"Accuracy: {:.1%}\".format(log_clf.score(X, y)))\n",
        "\n",
        "  # Print the accuracy for each tag\n",
        "  predictions = log_clf.predict(X)\n",
        "  errors = defaultdict(list)\n",
        "  for word, example, label, prediction in zip(word_list, X, y, predictions):\n",
        "    if label != prediction:\n",
        "      errors[label].append(word)\n",
        "\n",
        "  print(\"\\nAccuracy and first 10 errors per tag:\")\n",
        "  for tag, tag_words in zip(tags, words):\n",
        "    error_words = errors[tag]\n",
        "    num_total = len(tag_words)\n",
        "    num_correct = num_total - len(error_words)\n",
        "    ratio = num_correct / num_total\n",
        "    print(\"{:>3} {:,}/{:,} ({:.1%}) {}\".format(tag, num_correct, num_total, ratio,\n",
        "                                              \", \".join(error_words[:10])))\n",
        "\n",
        "# Download and prepare the Brown corpus for training and testing\n",
        "brown_train, brown_test = train_test_split(brown.tagged_sents(),\n",
        "                                           test_size=0.1,\n",
        "                                           random_state=42)\n",
        "\n",
        "print(\"Training sentences: {:,}\".format(len(brown_train)))\n",
        "print(\"Test sentences: {:,}\".format(len(brown_test)))\n",
        "\n",
        "# Get 1,000 examples of each tag\n",
        "tags = ['NP', 'NP$', 'VBG', 'VBD']\n",
        "random.seed(42)\n",
        "words = [random.sample(get_brown_tags(tag), 1000) for tag in tags]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sentences: 51,606\n",
            "Test sentences: 5,734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujpyyq8KGjN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f542740-a5cb-4dda-ba2b-dce1d3216be6"
      },
      "source": [
        "# Modify the features generated by this function and run the code cell to see\n",
        "# how your changes affect the accuracy of the classifier.\n",
        "def extract_features(word):\n",
        "  features = [\n",
        "      len(word),\n",
        "      word.endswith('s'),\n",
        "      word.endswith('es'),\n",
        "      word.endswith('ies'),\n",
        "      word.endswith('ed'),\n",
        "      word.endswith('ing'),\n",
        "      word.endswith(\"in'\"),\n",
        "      word[0].isupper(),\n",
        "      word.find(\"-\"),\n",
        "      word.find(\"'\"),\n",
        "      word.find(\"'d\"),\n",
        "      word.find(\"'s\"),\n",
        "      word.find(\"'re\")\n",
        "  ]\n",
        "\n",
        "  return features\n",
        "\n",
        "# The errors listed by this function are words belonging to that tag that were\n",
        "# incorrectly assigned with another tag. Use them to figure out useful features.\n",
        "train_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.4%\n",
            "\n",
            "Accuracy and first 10 errors per tag:\n",
            " NP 985/1,000 (98.5%) Whiting, Wilfred, niger, Diffring, aerogenes, Fred, anhemolyticus, Ring, Kooning, Sing\n",
            "NP$ 999/1,000 (99.9%) Grevyles\n",
            "VBG 999/1,000 (99.9%) waitin\n",
            "VBD 992/1,000 (99.2%) Exclaimed, Sat, Came, Became, Ran, Thought, Startled, Got\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiH2WoVmCyAV"
      },
      "source": [
        "## Question 3\n",
        "Word embeddings can capture semantic and syntactic relationships between words. For example, the vector between the words *king* and *man* is identical to the vector between *queen* and *woman* (i.e., *king* is to *man* as *queen* is to *woman*). This means that if we have a good vector representation for each of those words, we should be able to apply vector arithmetic to find that *king* - *man* + *woman* = *queen*.\n",
        "\n",
        "The function `find_word(a, b, x)`, defined below, finds the word **y**, such that **a** is to **b** as **x** is to **y** (also expressed as **a**:**b** as **x**:**y**).\n",
        "\n",
        "Below, we download GloVe word vectors through Gensim's API. Use those vectors and `find_words()` to complete the following tasks:\n",
        "1. In the UK, people say *petrol* instead of *gas*. Find the British English equivalent of the word *truck*.\n",
        "2. Find the capital of France.\n",
        "3. Find the present tense of the verb *flew*.\n",
        "\n",
        "**Note**: all words in `glove-wiki-gigaword-100` are in lowercase!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRFy2beIqXUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e117ce99-54bb-491e-fe2f-81c054e27cc7"
      },
      "source": [
        "import gensim.downloader as api\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def find_word(a, b, x):\n",
        "  # a is to b as x is to ?\n",
        "  a = a.lower()\n",
        "  b = b.lower()\n",
        "  x = x.lower()\n",
        "  print(f\"> {a}:{b} as {x}:?\")\n",
        "  top_words = glove.most_similar_cosmul(positive=[x, b], negative=[a])\n",
        "  for num, (word, score) in enumerate(top_words[:5]):\n",
        "    print(f\"{num + 1}: ({score:.3f}) {word}\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iqF6z-8okux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df52da9-e5ed-4225-fdab-19c5a45ebb4b"
      },
      "source": [
        "# Example 1: man is to king as woman is to ?\n",
        "find_word('man', 'king', 'woman')\n",
        "\n",
        "# Example 2: evening is to dinner as noon is to ?\n",
        "find_word('evening', 'dinner', 'noon')\n",
        "\n",
        "# 1) In the UK, people say 'petrol' instead of 'gas'. Find the British English\n",
        "# equivalent of 'truck'.\n",
        "find_word('gas', 'petrol', 'truck')\n",
        "\n",
        "\n",
        "# 2) Find the capital of France. Remember to use only lowercase characters.\n",
        "find_word('Italy', 'Rome', 'France')\n",
        "\n",
        "\n",
        "# 3) Find the present tense of the verb \"flew\".\n",
        "find_word('had', 'have', 'flew')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> man:king as woman:?\n",
            "1: (0.896) queen\n",
            "2: (0.850) monarch\n",
            "3: (0.845) throne\n",
            "4: (0.837) princess\n",
            "5: (0.836) elizabeth\n",
            "\n",
            "> evening:dinner as noon:?\n",
            "1: (0.839) lunch\n",
            "2: (0.829) breakfast\n",
            "3: (0.814) a.m.\n",
            "4: (0.814) p.m.\n",
            "5: (0.813) meal\n",
            "\n",
            "> gas:petrol as truck:?\n",
            "1: (1.020) lorry\n",
            "2: (0.957) wagon\n",
            "3: (0.951) trucks\n",
            "4: (0.950) lorries\n",
            "5: (0.945) car\n",
            "\n",
            "> italy:rome as france:?\n",
            "1: (0.995) paris\n",
            "2: (0.882) prohertrib\n",
            "3: (0.867) strasbourg\n",
            "4: (0.861) brussels\n",
            "5: (0.854) london\n",
            "\n",
            "> had:have as flew:?\n",
            "1: (0.892) fly\n",
            "2: (0.876) flying\n",
            "3: (0.870) flown\n",
            "4: (0.865) planes\n",
            "5: (0.842) flies\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-phpyFFCyjL"
      },
      "source": [
        "## Question 4\n",
        "Gensim offers us several ways to find words that are similar or dissimilar to one another. Complete the following tasks:\n",
        "1. Use `glove.most_similar(word, topn=5)` to find the five words that are most similar to:\n",
        "  1. cat\n",
        "  2. samsung\n",
        "  3. batman\n",
        "2. Use `glove.doesnt_match(list_of_strings)` to find which of the words below doesn't fit with the rest:\n",
        "  1. cat hamster gremlin rabbit goldfish dog\n",
        "  2. samsung microsoft dell panasonic mcdonalds facebook\n",
        "  3. batman spiderman daredevil shrek hulk deadpool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYGVE8bHCzA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5926965-52bb-44f5-ab16-75df71cbded4"
      },
      "source": [
        "from re import split\n",
        "words = [\"cat\", \"samsung\", \"batman\"]\n",
        "sentences = [\"cat hamster gremlin rabbit goldfish dog\",\n",
        " \"samsung microsoft dell panasonic mcdonalds facebook\",\n",
        "\"batman spiderman daredevil shrek hulk deadpool\"]\n",
        "\n",
        "for word in words:\n",
        "  print(f\"Similar words:{word}\\n\", glove.most_similar(word, topn = 5))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for i, word in enumerate(sentences):\n",
        "   print(glove.doesnt_match(sentences[i].split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words:cat\n",
            " [('dog', 0.8798074722290039), ('rabbit', 0.7424427270889282), ('cats', 0.732300341129303), ('monkey', 0.7288709878921509), ('pet', 0.719014048576355)]\n",
            "Similar words:samsung\n",
            " [('lg', 0.8194022178649902), ('toshiba', 0.7769339084625244), ('hyundai', 0.7322311401367188), ('fujitsu', 0.7246403694152832), ('panasonic', 0.7154008746147156)]\n",
            "Similar words:batman\n",
            " [('superman', 0.8058773279190063), ('superhero', 0.6820072531700134), ('sequel', 0.6592288017272949), ('catwoman', 0.654157817363739), ('joker', 0.6362104415893555)]\n",
            "\n",
            "\n",
            "gremlin\n",
            "mcdonalds\n",
            "shrek\n"
          ]
        }
      ]
    }
  ]
}