{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cA4f3BP_gZUC"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 1\n",
        "In these labs, we will be using the [Python 3](https://www.python.org/) programming language and the [Natural Language Toolkit (NLTK)](https://www.nltk.org/). We will also be using Google Colab, a free service hosted by Google, which gives us access to a Linux machine that comes pre-installed with Python 3 and the NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMY8d4CJ46K2"
      },
      "source": [
        "## Using Google Colab\n",
        "Google Colab allows users to work with \"notebooks\", which consists of text cells and code cells. Text cells can be edited by double clicking them. A code cell can be executed by selecting it and pressing `Ctrl + Enter`. Code is shared between cells, meaning that you can, for example, create a variable in one cell and use it in another cell later on.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run all of the code cells in the notebook.\n",
        "\n",
        "## Resources\n",
        "* [The Python Standard Library](https://docs.python.org/3/library/index.html) - an overview of the built-in libraries in Python with a lot of examples.\n",
        "* [The Python Tutorial](https://docs.python.org/3/tutorial/index.html) - an official tutorial that gives a brief overview of the language.\n",
        "* [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/) - a free book that offers a good introduction to the Python programming language to beginners.\n",
        "* [Natural Language Processing with Python](http://www.nltk.org/book/) - a free companion book to the NLTK toolkit.\n",
        "\n",
        "## Setting Python and the NLTK up on your own machine\n",
        "* [Python 3](https://realpython.com/installing-python/) - installation instructions\n",
        "* [NLTK](https://www.nltk.org/install.html) - installation instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f19uyslsaYw4"
      },
      "source": [
        "## String methods in Python\n",
        "There are many ways to manipulate strings in Python. A full list of methods for the String class may be found in the [library reference](https://docs.python.org/3/library/stdtypes.html#string-methods)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzTEm3RCgZUU",
        "outputId": "78bae95e-54b5-4fb8-c202-88736c810b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a_string = \"It was the best of times, it was the worst of times\"\n",
        "\n",
        "print(\"Lowercase:\", a_string.lower())\n",
        "print(\"'times' count:\", a_string.count('times'))\n",
        "print(\"First occurence of 'best':\", a_string.find('best'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase: it was the best of times, it was the worst of times\n",
            "'times' count: 2\n",
            "First occurence of 'best': 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmTHYN-agZVa"
      },
      "source": [
        "## Lists, sets and built-in functions\n",
        "Lists and sets are two kinds of collections that can be used in Python.\n",
        "* A **list** is an *ordered* sequence of elements. Lists are enclosed with square brackets, and the elements are separated with a comma (e.g., `a_list = [\"This\", \"is\", \"a\", \"list\"]`).\n",
        "* A **set** is a collection of *unordered* and *unique* elements (meaning that it contains no duplicates). Sets are enclosed by curly braces, and the elements are separated by commas (e.g., `a_set = {\"This\", \"is\", \"a\", \"set\"}`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8NWJM2rGgZVf",
        "outputId": "7104a3ac-2fa9-4dc5-d0f2-31b5cffd7e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Variables can be converted to lists and sets with the list() and set() functions\n",
        "char_list = list(a_string)\n",
        "char_set = set(a_string)\n",
        "\n",
        "# You can also split strings on certain characters to create a list of strings\n",
        "words = a_string.split()  # Splits on whitespaces by default\n",
        "print(\"Split string:\", words)\n",
        "\n",
        "# The len() and max() built-in methods\n",
        "print(\"Unique characters:\", char_set)\n",
        "print(\"No. of unique characters:\", len(char_set))\n",
        "print(\"Longest word:\", max(words, key=len))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split string: ['It', 'was', 'the', 'best', 'of', 'times,', 'it', 'was', 'the', 'worst', 'of', 'times']\n",
            "Unique characters: {'I', 'a', 'f', 'h', 's', 'b', ',', ' ', 'e', 'o', 'w', 'm', 'i', 'r', 't'}\n",
            "No. of unique characters: 15\n",
            "Longest word: times,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUt68ulEpfuw"
      },
      "source": [
        "You can use the built-in function `help()` to quickly access documentation for a given object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En1YvSC4qHJ8",
        "outputId": "2bfa49e8-350a-4b27-9b06-b79e121c7fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(max)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function max in module builtins:\n",
            "\n",
            "max(...)\n",
            "    max(iterable, *[, default=obj, key=func]) -> value\n",
            "    max(arg1, arg2, *args, *[, key=func]) -> value\n",
            "\n",
            "    With a single iterable argument, return its biggest item. The\n",
            "    default keyword-only argument specifies an object to return if\n",
            "    the provided iterable is empty.\n",
            "    With two or more arguments, return the largest argument.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X_TWoEUgZVw"
      },
      "source": [
        "## Indices and slicing\n",
        "You can get characters and substrings at specific indexes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p4o5Lc_gZV2",
        "outputId": "174dc14b-c668-47ae-f12a-7efd605abca8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"String:\", a_string)\n",
        "print(\"First character:\", a_string[0])  # Indices in Python start at 0\n",
        "print(\"Last character:\", a_string[-1])  # A negative index starts counting from the end\n",
        "\n",
        "# We can get ranges of elements by slicing a list\n",
        "print(\"Characters 11 to 14:\", a_string[11:15])\n",
        "print(\"First 6 characters:\", a_string[:6])\n",
        "print(\"Last 5 characters:\", a_string[-5:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String: It was the best of times, it was the worst of times\n",
            "First character: I\n",
            "Last character: s\n",
            "Characters 11 to 14: best\n",
            "First 6 characters: It was\n",
            "Last 5 characters: times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wQ9qvZ2gZV_"
      },
      "source": [
        "## The Natural Language Toolkit (NLTK)\n",
        "The NLTK book, [Natural Language Processing with Python](http://www.nltk.org/book/), is an introduction to natural language processing in Python, using the NLTK library. [Chapter 1](http://www.nltk.org/book/ch01.html) is relevant to this lab. The NLTK comes with a lot of data, such as corpora and trained models. We can download this data with the `nltk.download()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "txGWuokGgZWF",
        "outputId": "8c94748f-4d00-453d-87e8-4d2bf43d811c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Download the 'gutenberg' corpus, which is a collection of books in the public domain\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Get a plain-text version of Moby Dick (contained in a single string)\n",
        "moby_raw = gutenberg.raw('melville-moby_dick.txt')\n",
        "\n",
        "# Get all the tokens in Moby Dick (as a list of strings)\n",
        "moby_tokens = gutenberg.words('melville-moby_dick.txt')\n",
        "\n",
        "# Print the first 10 tokens of Moby Dick\n",
        "print(\"First 10 tokens:\", moby_tokens[:10])\n",
        "\n",
        "# Print the first 250 characters of Moby Dick\n",
        "print(\"\\nFirst 250 characters:\\n\", moby_raw[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 tokens: ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.']\n",
            "\n",
            "First 250 characters:\n",
            " [Moby Dick by Herman Melville 1851]\r\n",
            "\r\n",
            "\r\n",
            "ETYMOLOGY.\r\n",
            "\r\n",
            "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
            "\r\n",
            "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
            "now.  He was ever dusting his old lexicons and grammars, with \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk7r17iXgZW-"
      },
      "source": [
        "NLTK includes a Text class for analyzing the contents of texts. Let's print a concordance for the word *Iceland*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCPcW0p9gZXB",
        "outputId": "a5bf8e74-2fd8-491a-e57b-aaaa84285b56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "moby_text = nltk.Text(moby_tokens)\n",
        "moby_text.concordance('Iceland')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 4 of 4 matches:\n",
            "ANKS ' S AND SOLANDER ' S VOYAGE TO ICELAND IN 1772 . \" The Spermacetti Whale f\n",
            " an adjoining room . It was cold as Iceland -- no fire at all -- the landlord s\n",
            " ? Throw yourselves ! Legs ! legs ! ICELAND SAILOR . I don ' t like your floor \n",
            "my of Sciences setting down certain Iceland Whales ( reydan - siskur , or Wrink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntDr5ACsgZXM"
      },
      "source": [
        "NTLK offers several ways to segment text into sentences and tokenize it. Let's see how `nltk.sent_tokenize()` and `nltk.word_tokenize()` work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IZ1nLySgZXQ",
        "outputId": "ee4e7be4-d4ed-4008-8e96-4d628983c569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download the Punkt tokenizer model\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "moby_sentences = nltk.sent_tokenize(moby_raw)  # Split raw text into sentences\n",
        "tokens = nltk.word_tokenize(moby_sentences[3])  # Split a string into tokens\n",
        "\n",
        "print(\"First 5 sentences:\")\n",
        "for sentence in moby_sentences[:5]:\n",
        "    print(\">>>\", sentence)\n",
        "\n",
        "print(f\"\\nTotal number of sentences: {len(moby_sentences):,}\")\n",
        "\n",
        "print(\"\\nTokens:\", tokens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences:\n",
            ">>> [Moby Dick by Herman Melville 1851]\r\n",
            "\r\n",
            "\r\n",
            "ETYMOLOGY.\n",
            ">>> (Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
            "\r\n",
            "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
            "now.\n",
            ">>> He was ever dusting his old lexicons and grammars, with a queer\r\n",
            "handkerchief, mockingly embellished with all the gay flags of all the\r\n",
            "known nations of the world.\n",
            ">>> He loved to dust his old grammars; it\r\n",
            "somehow mildly reminded him of his mortality.\n",
            ">>> \"While you take in hand to school others, and to teach them by what\r\n",
            "name a whale-fish is to be called in our tongue leaving out, through\r\n",
            "ignorance, the letter H, which almost alone maketh the signification\r\n",
            "of the word, you deliver that which is not true.\"\n",
            "\n",
            "Total number of sentences: 9,852\n",
            "\n",
            "Tokens: ['He', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC_OW-lUgZXo"
      },
      "source": [
        "## Regular expressions\n",
        "The Python standard library includes the `re` module for handling regular expressions ([reference](https://docs.python.org/3/library/re.html)). In Python, regular expression patterns should be created using *raw* strings, which are prefixed with an `r`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "I7cDMt90gZXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32afa90-a0b2-4800-b1b9-267b2111dfa2"
      },
      "source": [
        "import re\n",
        "re.findall(r'\\b\\S{9,}est\\b', moby_raw)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sovereignest',\n",
              " 'off--serenest',\n",
              " 'monstrousest',\n",
              " 'Wonderfullest',\n",
              " 'surrenderest',\n",
              " 'cowardly--quickest']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z_417IcgZX1"
      },
      "source": [
        "You can capture text that matches a specific part of a pattern in a group by enclosing it within parentheses. When making substitutions with `re.sub()`, you can refer to these groups using `\\number` (e.g., `\\1` and `\\2`), where the number refers to their position in the pattern:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHyZVF5pgZX3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "545f0a0e-7df9-43d0-f7cb-98f16ce30118"
      },
      "source": [
        "another_string = \"The grapes of wrath\"\n",
        "re.sub(r'(\\S+) of (\\S+)', r'\\2 of \\1', another_string)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The wrath of grapes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-U35BXYgZYD"
      },
      "source": [
        "NLTK offers a simple way of searching for sequences of tokens using regular expressions, where tokens can be enclosed in angle brackets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "pRChhPklgZYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f8c2a6-a045-4768-e0d5-72e79b7d0711"
      },
      "source": [
        "# First we must create a Text object from a list of tokens\n",
        "moby_text = nltk.Text(moby_tokens)\n",
        "\n",
        "# Let's search for sequences of four tokens which all begin with the letter S\n",
        "moby_text.findall(r'<[Ss].*>{4,}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ship so swiftly sped; snatch some sweet solace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIt3E060gZYQ"
      },
      "source": [
        "You can use groups to target specific tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3L3nrg2SgZYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9acda1e4-a43e-4528-ae65-ff64a306c72f"
      },
      "source": [
        "moby_text.findall(r'<[Aa]n?>(<.+>)<ship>')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full; gallant; whale; modern; trading; large; Nantucket; leaking;\n",
            "midnight; wrecked; occupied; fine; jolly; empty; great; large; leaky;\n",
            "Nantucket; full; full; empty; whole; large; sagacious\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksz_n_cwgZYb"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before midnight, August 30th. Make a copy of this notebook (`\"File\" > \"Save a copy in Drive\"`) and enter your solutions in the cells below each question. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEcJJbIUgZYi"
      },
      "source": [
        "### Question 1\n",
        "Get the raw text of `carroll-alice.txt` (Alice in Wonderland) from the Gutenberg corpus in NLTK and tokenize it using `nltk.word_tokenize()`.\n",
        "\n",
        "1. How many tokens does it contain in total?\n",
        "2. How many unique tokens (types) does it contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "dsA5lgp0gZYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de75f536-ac88-4073-850e-a3d32a0244f6"
      },
      "source": [
        "nltk.download(\"gutenberg\")\n",
        "\n",
        "alice_raw = gutenberg.raw(\"carroll-alice.txt\")\n",
        "alice_tokens = nltk.word_tokenize(alice_raw)\n",
        "\n",
        "total_tokens_list = list()      #Intialize total tokens variable\n",
        "total_tokens_set = set()\n",
        "\n",
        "alice_sentences = nltk.sent_tokenize(alice_raw)   #Tokenize all the text into a sentences\n",
        "\n",
        "# For every sentence, tokenize all the words in the following sentence and\n",
        "# add the number to the total_tokens variable\n",
        "\n",
        "for sentence in alice_sentences[:len(alice_sentences)]:\n",
        "  alice_current_list = list(nltk.word_tokenize(sentence))\n",
        "  alice_current_set = set(nltk.word_tokenize(sentence))\n",
        "\n",
        "  total_tokens_list.extend(alice_current_list)\n",
        "  total_tokens_set.update(alice_current_set)\n",
        "\n",
        "print(f\"\\nTotal tokens number: {len(total_tokens_list):,}\")\n",
        "print(f\"Total unique tokens number: {len(total_tokens_set):,}\")\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total tokens number: 33,535\n",
            "Total unique tokens number: 3,157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9OMFNxgZY6"
      },
      "source": [
        "### Question 2\n",
        "Use `nltk.FreqDist()` to create a frequency distribution of all the tokens in Alice in Wonderland. What are the 20 most frequently occurring tokens?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "tOPA_dljgZY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5e967a-3e1f-4b9d-df04-7d8e0eece4c7"
      },
      "source": [
        "alice_freq = nltk.FreqDist(total_tokens_list)\n",
        "\n",
        "freq_mostcommonlist = list(alice_freq.most_common(20))\n",
        "\n",
        "for index , freq in enumerate(freq_mostcommonlist):\n",
        "  print(f\"{index + 1} word: {freq}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 word: (',', 2418)\n",
            "2 word: ('the', 1516)\n",
            "3 word: (\"'\", 1311)\n",
            "4 word: ('.', 975)\n",
            "5 word: ('and', 757)\n",
            "6 word: ('to', 717)\n",
            "7 word: ('a', 614)\n",
            "8 word: ('I', 543)\n",
            "9 word: ('it', 513)\n",
            "10 word: ('she', 507)\n",
            "11 word: ('of', 496)\n",
            "12 word: ('said', 456)\n",
            "13 word: ('!', 450)\n",
            "14 word: ('Alice', 395)\n",
            "15 word: ('was', 362)\n",
            "16 word: ('in', 352)\n",
            "17 word: ('you', 337)\n",
            "18 word: ('that', 267)\n",
            "19 word: ('--', 264)\n",
            "20 word: ('her', 243)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ku3fBv6gZZH"
      },
      "source": [
        "### Question 3\n",
        "Use `nltk.sent_tokenize()` to segment Alice in Wonderland into sentences, then find the longest sentence in the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "-EuYbFqJgZZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8a3618-6d67-4bc8-e984-a8726ec45ce6"
      },
      "source": [
        "print(f\"The longest sentece in the book:\\n {max(alice_sentences, key=len)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest sentece in the book:\n",
            " Hardly knowing what she did, she picked up a little bit of stick, and\n",
            "held it out to the puppy; whereupon the puppy jumped into the air off\n",
            "all its feet at once, with a yelp of delight, and rushed at the stick,\n",
            "and made believe to worry it; then Alice dodged behind a great thistle,\n",
            "to keep herself from being run over; and the moment she appeared on the\n",
            "other side, the puppy made another rush at the stick, and tumbled head\n",
            "over heels in its hurry to get hold of it; then Alice, thinking it was\n",
            "very like having a game of play with a cart-horse, and expecting every\n",
            "moment to be trampled under its feet, ran round the thistle again; then\n",
            "the puppy began a series of short charges at the stick, running a very\n",
            "little way forwards each time and a long way back, and barking hoarsely\n",
            "all the while, till at last it sat down a good way off, panting, with\n",
            "its tongue hanging out of its mouth, and its great eyes half shut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExlDi0jDgZZR"
      },
      "source": [
        "### Question 4\n",
        "Use a regular expression to find all tokens in Alice in Wonderland that contain an *x* and end with *ed*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtfpRxu5gZZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81569092-0c71-4a24-fd15-c4cc0fa4453a"
      },
      "source": [
        "alice_text = nltk.Text(alice_tokens)\n",
        "alice_text.findall(r'<.*x.*ed>')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mixed; fixed; executed; expected; exclaimed; boxed; executed;\n",
            "executed; exclaimed; exclaimed; exclaimed; explained; exclaimed;\n",
            "executed; executed; executed; exclaimed; mixed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0rG5i_VgZZa"
      },
      "source": [
        "### Bonus Question\n",
        "\n",
        "Use `re.sub()` to \"dehyphenate\" the following string:\n",
        "\n",
        ">It is a capital mistake to theo-  \n",
        ">rize before one has data. Insen-  \n",
        ">sibly one begins to twist facts  \n",
        ">to suit theories, instead of the-  \n",
        ">ories to suit facts.\n",
        "\n",
        "You will need to use groups to recombine the words. The resulting string should look like this:\n",
        "\n",
        ">It is a capital mistake to  \n",
        ">theorize before one has data.  \n",
        ">Insensibly one begins to twist facts  \n",
        ">to suit theories, instead of  \n",
        ">theories to suit facts.\n",
        "\n",
        "Remember that a \"newline\" character is represented by `\\n` in strings and regular expression patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "zKyWgOPkgZZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f048a1d-3e0f-43aa-eaec-7f3d62170563"
      },
      "source": [
        "hyphenated = \"\"\"\n",
        "It is a capital mistake to theo-\n",
        "rize before one has data. Insen-\n",
        "sibly one begins to twist facts\n",
        "to suit theories, instead of the-\n",
        "ories to suit facts.\n",
        "\"\"\"\n",
        "\n",
        "print(re.sub(r'(\\S+)-\\n(\\S+)', r'\\n\\1\\2', hyphenated))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "It is a capital mistake to \n",
            "theorize before one has data. \n",
            "Insensibly one begins to twist facts\n",
            "to suit theories, instead of \n",
            "theories to suit facts.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}